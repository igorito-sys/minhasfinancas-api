<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Red Hat Developer roundup: Best of July 2022</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/29/red-hat-developer-roundup-best-july-2022" /><author><name>Heiker Medina</name></author><id>34b403a3-f94b-4fe0-bca0-662f87d23cff</id><updated>2022-07-29T07:00:00Z</updated><published>2022-07-29T07:00:00Z</published><summary type="html">&lt;p&gt;Welcome to our monthly article recap, where we round up the latest popular content from Red Hat Developer in one helpful place. Like &lt;a href="https://developers.redhat.com/articles/2022/06/30/red-hat-developer-roundup-best-june-2022"&gt;last month&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; &lt;a href="https://developers.redhat.com/topics/security/"&gt;security&lt;/a&gt; topics were in high demand from our readers. Without further ado, let's dive into the July highlights.&lt;/p&gt; &lt;h2&gt;GitOps workflows and security&lt;/h2&gt; &lt;p&gt;In GitOps, Git is not only your source of truth (as it is for most projects) but also your interface to your environment. Developers have used Git workflows for their application delivery method for years, and now operations teams must adopt similar workflows. GitOps advocate Christian Hernandez offered &lt;a href="https://developers.redhat.com/articles/2022/07/20/git-workflows-best-practices-gitops-deployments"&gt;tips and best practices to keep in mind for GitOps deployments&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Interested in diving deeper into GitOps principles? Download Christian's new e-book, &lt;a href="https://developers.redhat.com/e-books/path-gitops"&gt;&lt;em&gt;The Path to GitOps&lt;/em&gt;&lt;/a&gt;, to understand where GitOps fits in your &lt;a href="https://developers.redhat.com/topics/linux/"&gt;continuous integration/continuous delivery (CI/CD)&lt;/a&gt; pipelines. This short guide outlines the various tools and methods you can use to implement GitOps in your organization.&lt;/p&gt; &lt;p&gt;Sahil Sethi also showed you &lt;a href="https://developers.redhat.com/articles/2022/07/11/deploy-operator-gitops-using-advanced-cluster-management"&gt;how to integrate security policies into a GitOps environment&lt;/a&gt; to apply consistently throughout your clusters. Security policies are part of &lt;a href="https://www.redhat.com/en/technologies/management/advanced-cluster-management"&gt;Red Hat Advanced Cluster Management for Kubernetes&lt;/a&gt;, a platform that helps users configure and deploy applications and other valuable services such as metrics. &lt;/p&gt; &lt;h2&gt;SaaS security&lt;/h2&gt; &lt;p&gt;The most recent article of our series on &lt;a href="https://developers.redhat.com/articles/2022/05/18/saas-architecture-checklist-kubernetes"&gt;building and deploying Software as a service (SaaS) applications&lt;/a&gt; covered &lt;a href="https://developers.redhat.com/articles/2022/07/27/saas-security-kubernetes-environments-layered-approach"&gt;SaaS security for containers in Kubernetes environments&lt;/a&gt;. Within modern enterprise environments, it's critical to build security into the full life cycle of planning, development, operations, and maintenance.&lt;/p&gt; &lt;p&gt;Keep an eye out for future installments covering &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; for SaaS development, scalability and disaster recovery, and more.&lt;/p&gt; &lt;h2&gt;Integrate Infinispan and ASP.NET Core&lt;/h2&gt; &lt;p&gt;The open source Infinispan data store is a popular option for in-memory operations. &lt;a href="https://developers.redhat.com/topics/dotnet/"&gt;ASP.NET Core&lt;/a&gt; applications can now easily integrate Infinispan as a caching service or session provider. Vittorio Rigamonti provides answers on how to do that in &lt;a href="https://developers.redhat.com/topics/c/"&gt;C#&lt;/a&gt; on &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; in his article &lt;a href="https://developers.redhat.com/articles/2022/07/07/add-infinispan-cache-your-aspnet-application"&gt;Add an Infinispan cache to your ASP.NET application&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Go Toolset container images&lt;/h2&gt; &lt;p&gt;Red Hat's Go Toolset package is available as a container image and offers developers a kickstart to building modern Go applications. It delivers the Go language with Federal Information Processing Standard (FIPS) support for cryptographic modules and the Delve debugger to &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; customers.&lt;/p&gt; &lt;p&gt;In &lt;a href="https://developers.redhat.com/articles/2022/07/21/how-use-go-toolset-container-images"&gt;How to use Go Toolset container images&lt;/a&gt;, Alejandro Sáez Morollón illustrates how these images support modern Go development and make you more productive in the cloud.&lt;/p&gt; &lt;h2&gt;Secure secrets on Kubernetes &lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2019/08/15/how-to-use-dekorate-to-create-kubernetes-manifests/"&gt;Dekorate&lt;/a&gt; is a tool that simplifies the process of generating cert-manager custom resources. You can use it to create secrets, such as encryption keys and passwords for your Spring Boot application.&lt;/p&gt; &lt;p&gt;Read &lt;a href="https://developers.redhat.com/articles/2022/07/19/secure-kubernetes-certificates-cert-manager-and-dekorate"&gt;Secure Kubernetes certificates with cert-manager and Dekorate&lt;/a&gt; and learn how to keep your secrets safe while developing or running production applications.&lt;/p&gt; &lt;h2&gt;July 2022 on Red Hat Developer&lt;/h2&gt; &lt;p&gt;Here's the full lineup of articles published on Red Hat Developer this month:&lt;/p&gt; &lt;ul&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/27/saas-security-kubernetes-environments-layered-approach"&gt;SaaS security in Kubernetes environments: A layered approach&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/21/how-use-go-toolset-container-images"&gt;How to use Go Toolset container images&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/20/git-workflows-best-practices-gitops-deployments"&gt;Git workflows: Best practices for GitOps deployments&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/19/secure-kubernetes-certificates-cert-manager-and-dekorate"&gt;Secure Kubernetes certificates with cert-manager and Dekorate&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/18/simplify-client-connection-configurations-service-contexts"&gt;Connect to OpenShift application services with contexts&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/15/new-http-clients-java-generator-and-more-fabric8-600"&gt;New HTTP clients, a Java generator, and more in Fabric8 6.0.0&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/14/kafka-monthly-digest-june-2022"&gt;Kafka Monthly Digest: June 2022&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/07/add-infinispan-cache-your-aspnet-application"&gt;Add an Infinispan cache to your ASP.NET application&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/06/deploy-jboss-eap-microsoft-azure-red-hat-openshift"&gt;Deploy JBoss EAP on Microsoft Azure Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/06/what-qualifies-red-hat-developer-subscription-teams"&gt;What qualifies for Red Hat Developer Subscription for Teams?&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/05/debugging-hedy-and-nostalgia-3-talks-openjs-world-2022"&gt;Debugging, Hedy, and nostalgia: 3 talks at OpenJS World 2022&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/05/write-systemtap-script-trace-code-execution-linux"&gt;Write a SystemTap script to trace code execution on Linux&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/13/install-storage-your-application-cluster-using-rook"&gt;Install storage in your application cluster using Rook&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/12/how-run-vs-code-openshift-dev-spaces"&gt;How to run VS Code with OpenShift Dev Spaces&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/11/deploy-operator-gitops-using-advanced-cluster-management"&gt;Deploy an Operator via GitOps using Advanced Cluster Management&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/29/red-hat-developer-roundup-best-july-2022" title="Red Hat Developer roundup: Best of July 2022"&gt;Red Hat Developer roundup: Best of July 2022&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Heiker Medina</dc:creator><dc:date>2022-07-29T07:00:00Z</dc:date></entry><entry><title type="html">Keycloak 19.0.1 released</title><link rel="alternate" href="https://www.keycloak.org/2022/07/keycloak-1901-released" /><author><name /></author><id>https://www.keycloak.org/2022/07/keycloak-1901-released</id><updated>2022-07-29T00:00:00Z</updated><content type="html">To download the release go to . MIGRATION FROM 18.0 Before you upgrade remember to backup your database. If you are not on the previous release refer to for a complete list of migration changes. ALL RESOLVED ISSUES BUGS * Set `resourcesVersionSeed` when using ConcurrentHashMap storage keycloak storage * Installation error - You have an error in your SQL syntax keycloak dist/quarkus * Documentation fixes in configuring keycloak page keycloak dist/quarkus UPGRADING Before you upgrade remember to backup your database and check the for anything that may have changed.</content><dc:creator /></entry><entry><title type="html">Efesto refactoring &amp;#8211; Introduction</title><link rel="alternate" href="https://blog.kie.org/2022/07/efesto-refactoring-introduction.html" /><author><name>Gabriele Cardosi</name></author><id>https://blog.kie.org/2022/07/efesto-refactoring-introduction.html</id><updated>2022-07-28T07:10:46Z</updated><content type="html">This post is meant as an introduction of the overall motivations, goals and choices around the Efesto initiative. PREMISE Originally, "Drools" (and its repository) was meant only as a "rule engine", and all the code was built around this paradigm. Over the years, new engines have been created that used, more or less, the "rule engine", or even not at all. That changed completely the actual paradigm, but the code did not reflected such a change. Different solutions or workarounds have been put in place to make this two incompatible realities (a code meant to invoke mainly the "rule engine", on one side, and different engines interacting in a coordinate manner, on the other side) works together. One of this attempt was the introduction of the "KieAssembler" (and derived) APIs. The goal was to provide a way to coordinate the execution of the different engines, but unfortunately the implementation had two flaws: * its execution has been inserted inside the code flow that was originally written for the rule engine; * it has not been adapted by all the engines, but only by the ones developed after its introduction. The result of the above is that what was meant as "coordinator" of all the different engines, became a specific sub-path of execution of the rules one; and the path of execution of the rules became, as a matter of fact, the coordinator of all the other engines. Beside that, for reasons specific to the "rule engine", the separation of a "compilation" phase and an "execution" one has never been strongly enforced, and this lack of separation leaked in the codebase. The post provides details of an analogue work done inside the Rule engine itself, showing the complexity of the task to be faced. These issues made the code hard to maintain and to expand, requiring a lot of ad-hoc solutions for problems that, actually, are inherent to the whole system. The "KieAssembler" clearly shows that. The engines that extend it have to implement the methods needed for both the compilation and the execution phase; and those KieAssembler-extending classes are invoked both at compile-time and at runtime-phase. As an example, the currently available version of tries to enforce a kind of separation with two different utility classes: * * Again, this is a downstream workaround for a design flaw. As such, each engine should write similar workarounds, and that would not solve the root cause. Consequence of that is that different engines follows different designs and address the same needs in different ways. Some attempts have been made to address these shortcomings, but at a downstream level, and for a more or less specific use-case, making those attempts less efficient then expected. The best example of this is the project. The goal of the "Efesto" refactoring is to tackle all the mentioned issues at the root, adapting the overall codebase to the current paradigm by which the different components are used, following the hard lessons learned over the years. DICTIONARY We define a domain dictionary here because, over the years, some terms have been used with different meanings in different situations, and almost always misunderstanding arose due to these different interpretations. The following are the definition and meanings used in this series of posts: * Model: the textual representation of a given model; e.g. Rules (DRL, other), Decision (DMN), Predictions (PMML), Workflow (BPMN, other) * Engine: the code needed to * transform a specific model in executable form; * execute the executable form of a specific model with a given input data * some examples: * Rule engine * Decision engine * Prediction engine * Workflow engine * Efesto: the framework that exposes the functionalities of the different engines and the name of the project that contains the refactoring * Compile-time: the process of transform the original model in executable form * Runtime: the process of executing a given model with a user input * Container: a given application that uses the drools functionalities (compilation and/or runtime) to fulfill its scope; some examples: * Kie-maven-plugin: uses compile-time to retrieve bytecode and then dump it to a kjar; * Kie-server: uses runtime to load/execute kjars (and, eventually, compile-time for on-the-fly compilation/reload) * Kogito-build: uses compile-time to retrieve bytecode and then dump it to a jar/native image; * Kogito-execution: uses runtime to load/execute jar/native image CLEAN ARCHITECTURE PRINCIPLES The main goal is to have a modular, decoupled system that will be easy to maintain in the long term (i.e. fixing bugs, improving performance, adding features). To achieve that, the knowledge relationship between the different parts is clearly defined and enforced. The system has core components and peripheral components. The “knowledge” arrow points only inward, i.e. peripheral components have knowledge of core components, but not the other way around. Peripheral components does not have knowledge of each other. MICROKERNEL STYLE The microkernel/plugin design is used to reflect the relationship between the different engines and the overall system. Every engine is implemented as a plugin component, and no direct relationship exists between plugins. MAIN TASKS The goal of Efesto refactoring are: * Separate what is “Drools” and what is not Drools * Separate compilation/execution phases * Enforce engines consistency * Provide a pluggable/chainable design SEPARATE DROOLS/NOT DROOLS Efesto (the framework, as defined in this post) is considered an agnostic provider of model execution. As such, it does not depend on any other framework, and it is available as a standalone library, runnable inside any kind of environment/container (e.g. Spring, Quarkus, Kogito, KieServer, etc). To allow that, it contains the bare-minum code required to coordinate the transformation of models in unit of executions, and the execution of them to provide a result. One consequence of this approach is that some functionalities, that are currently in charge of the drools code, will be delegated to the "container". As example, the framework does not write compiled classes to the filesystem, but delegates this task to the invoking code, like the KieMaven plugin. The reason behind this specific choice is that write to a filesystem, and relying on that, requires a series of assumptions (firt of all, a read-write environment) that are not absolutely granted, and should not be addressed by the framework itself, but by the container it is used in. SEPARATE COMPILATION/EXECUTION PHASES As defined before, compilation is the process of transforming a model to an executable unit. Usually it involves some code-generation, but this is not mandatory at all. The result of a compilation is stored inside a so-called "IndexFile", that is a registry of the generated resources, and also contains the entry-point for the execution. As a matter of fact, this entry-point could be a code-generated class, but also an already-existing one (e.g. DMN). On the other side, execution is the process of receiving input data, submitting it to unit of execution, and returning a result. In this phase, the framework reads the identifier of the resource to be invoked from the input; then, the required engine reads the informations needed for the invocation of the entry point from the IndexFile. ENFORCE ENGINES CONSISTENCY Every engine follows the same design. This means that inside the Drools framework there is not a preferential path of execution, tailored around one specific engine, to which all the others have to adapt. Instead, they all implements the same common API, so that the flow of execution is the same for every one. At the same time, this requires and enforces independency between the engines. Every engine implements a “compilation” service and a “loading” service: the former responsible of compiled-resource generation (e.g. code-generation, class compilation, entry-point definition); the latter responsible for actual entry-point invocation. PROVIDE A PLUGGABLE/CHAINABLE DESIGN The microkernel architecture allows the implementation of different engines as isolated plugins. That, in turns, provides some out-of-the-box features: * parallel development of different engines, avoiding overlapping/conflict issues * incremental implementation of new engines, without the needs of a BigBang release * no Monolithic design, where every component is bound, directly on indirectly, to the others * different implementation for the same engine, delegating the choice to the container (with the maven dependency mechanism) * allows “customer” to implement their own version/customization for a given engine The "chainable" feature refers to the possibility to invoke one engine from another. This is a well-known requirement at execution time (e.g. DMN engine requires PMML engine evaluation), but also at compile-time. Since part of execution could be delegated to another engine, this implies that the invoked engine should have "compiled" that part of execution (whatever this mean in specific cases). Another interesting use case is to compile different resources to the same engine. An example of this is offered by Rule engine. THE RULE ENGINE USE-CASE The Rule engine actually has different "formats": Drl files, Decision tables, etc.. All this models are "translated" to a PackageDescr at a given point; and the final result is always the same, an Executable model. For each kind of source there is a specific implementation responsible to translate it to a PackageDescr. There is also an implementation that takes as input the PackageDescr and returns the Executable model. So, the different model-specific engines translates the input to a PackageDescr, and then delegates to the latter one to transform it to the final Executable model. As a by-side note, that chainability feature provides an extremely easy and fast way to manage any kind of "definition" as "Rules" (or whatever engine). CONCLUSION This is the first post of a series around the Efesto effort and implementation. Following ones will go deeper inside technical details and will provide some real use-cases and code so… stay tuned!!! The post appeared first on .</content><dc:creator>Gabriele Cardosi</dc:creator></entry><entry><title type="html">How to run Java Mission Control in Eclipse</title><link rel="alternate" href="http://www.mastertheboss.com/java/how-to-run-java-mission-control-in-eclipse/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/how-to-run-java-mission-control-in-eclipse/</id><updated>2022-07-27T15:02:27Z</updated><content type="html">This article continues our learning through the Java Mission Control (JMC) tool. Within it, we will learn how to run JMC as standalone application or as Eclipse IDE plugin. Firstly, if you are new to Java Mission Control, we recommend checking this article for a brief introduction to it: How to use Java Mission Control ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>SaaS security in Kubernetes environments: A layered approach</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/27/saas-security-kubernetes-environments-layered-approach" /><author><name>Alex Kubacki</name></author><id>265fb1fc-a3c2-44d1-a2d0-41d140c6ac79</id><updated>2022-07-27T07:00:00Z</updated><published>2022-07-27T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/security/"&gt;Security&lt;/a&gt; is especially critical for &lt;a href="https://www.redhat.com/en/topics/cloud-computing/what-is-saas"&gt;Software-as-a-Service (SaaS)&lt;/a&gt; environments, where the platform is used by many different people who need the confidence that their data is stored safely and kept private from unrelated users. This article focuses on security concerns for &lt;a data-entity-substitution="canonical" data-entity-type="node" data-entity-uuid="4093cbb4-5a74-4678-9a7a-7e3d9c8b81c1" href="https://developers.redhat.com/topics/containers" title="Building containerized applications"&gt;containers&lt;/a&gt; on your SaaS deployment running in &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; environments such as &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. The article is the fifth in a series called the &lt;a href="https://developers.redhat.com/articles/2022/05/18/saas-architecture-checklist-kubernetes"&gt;SaaS architecture checklist&lt;/a&gt; that covers the software and deployment considerations for SaaS applications.&lt;/p&gt; &lt;h2&gt;Security controls and practices for SaaS&lt;/h2&gt; &lt;p&gt;Within modern enterprise environments, security needs to be built into the full life cycle of planning, development, operations, and maintenance. Good security controls and practices are critical to meeting compliance and regulatory requirements and making sure that transactions are reliable and high-performing. Security in SaaS can be broken down into five main layers: hardware, operating system, containers, Kubernetes, and networking. Figure 1 shows these layers and the security controls that address threats at each layer.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/layers.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/layers.png?itok=5BHUmoFN" width="478" height="829" alt="SaaS layers and their security features in Kubernetes and OpenShift." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: SaaS layers and their security features in Kubernetes and OpenShift.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Security needs to be addressed at every layer because any vulnerability in one layer could be exploited to compromise other layers. For each layer, Kubernetes and OpenShift have security controls and features that will be covered in this article. Future articles will go into more detail on specific SaaS security topics. If there are any SaaS topics for which you would like to see an article, let us know in the comments.&lt;/p&gt; &lt;h2&gt;Security at the hardware layer&lt;/h2&gt; &lt;p&gt;Securing a SaaS environment often starts with identifying where the application is going to run and the security concerns for that environment. A secure environment includes the actual data center as well as the hardware itself, including disk encryption, secure boot, BIOS-level passwords, and the use of hardware security modules (HSMs). Secrets and identity management are discussed later in this article.&lt;/p&gt; &lt;p&gt;While a lot of attention is paid to using encryption to protect &lt;em&gt;data in transit&lt;/em&gt; as it goes over the network, it is also critical to protect &lt;em&gt;data at rest&lt;/em&gt; as it is stored on physical storage devices in data centers. The risks to data at rest are much higher in data centers where you lack control over access to the facility and where third-party contractors may be employed. Use disk encryption to secure data at rest by protecting the data stored on the physical server from unintended access.&lt;/p&gt; &lt;p&gt;An HSM is typically a physical device that securely stores digital keys through encryption to protect sensitive data. HSMs are used to manage and safeguard security credentials, keys, certificates, and secrets while at rest and in transit. The HSM provides an increased level of protection over software-only approaches such as a secrets vault.&lt;/p&gt; &lt;p&gt;Cloud HSMs are available from the major cloud providers to provide increased protection in cloud environments. HSMs are recommended to manage secrets in SaaS environments.&lt;/p&gt; &lt;p&gt;Protect access to the server by enabling secure boot and using BIOS-level passwords. Secure boot is a firmware security feature of the Unified Extensible Firmware Interface (UEFI) that makes sure that only immutable and signed software can be run during boot.&lt;/p&gt; &lt;p&gt;For more information, check out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/identity-and-access-devsecops-life-cycle"&gt;Identity and access in the DevSecOps life cycle&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://cloud.redhat.com/blog/self-contained-ready-and-secured-enhancing-red-hat-openshift-with-hardware-cryptography"&gt;Enhancing Red Hat OpenShift with Hardware Cryptography&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/security_hardening/assembly_securing-rhel-during-installation-security-hardening#BIOS_and_UEFI_security_securing-rhel-during-installation"&gt;Securing Red Hat Enterprise Linux during installation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Operating system security&lt;/h2&gt; &lt;p&gt;Every Kubernetes cluster runs on top of some underlying operating system (OS). Security features and hardening at the OS layer help protect the overall cluster, so it is important to enable and use OS-level controls.&lt;/p&gt; &lt;p&gt;When it comes to security hardening at the OS level, Red Hat OpenShift has two distinct advantages. First, &lt;a href="https://www.redhat.com/en/topics/linux/what-is-selinux"&gt;Security-Enhanced Linux&lt;/a&gt; (SELinux) is integrated and enabled out of the box. Second, OpenShift runs on Red Hat Enterprise Linux CoreOS, a unique OS image tuned for SaaS use.&lt;/p&gt; &lt;h3&gt;Security-enhanced Linux&lt;/h3&gt; &lt;p&gt;SELinux is a security architecture for &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; systems that grants administrators finer-grained control over access to system resources than is available with default Linux. SELinux defines mandatory access controls for applications, processes, and files on a system. On a Kubernetes node, SELinux adds an important layer of protection against &lt;a href="https://www.redhat.com/en/blog/latest-container-exploit-runc-can-be-blocked-selinux"&gt;container-breakout vulnerabilities&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Thus, one of the most effective security measures is to enable and configure SELinux, which Red Hat has made standard on all OpenShift clusters. It is considered a best practice to use SELinux in SaaS environments. In OpenShift, SELinux enhances container security by ensuring true container separation and mandatory access control.&lt;/p&gt; &lt;p&gt;For more information, see:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/how-selinux-separates-containers-using-multi-level-security"&gt;How SELinux separates containers using Multi-Level Security&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/why-you-should-be-using-multi-category-security-your-linux-containers"&gt;Why you should be using Multi-Category Security for your Linux containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/using-container-technology-make-trusted-pipeline"&gt;Using container technology to make a more secure pipeline&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/network-traffic-control-containers-red-hat-openshift"&gt;Network traffic control for containers in Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;A hardened OS for containers: Red Hat Enterprise Linux CoreOS&lt;/h3&gt; &lt;p&gt;OpenShift's operating system, Red Hat Enterprise Linux CoreOS, is based on Red Hat Enterprise Linux and uses the same kernel, code, and open source development processes. This special version ships with a specific subset of Red Hat Enterprise Linux packages, designed for use in OpenShift 4 clusters. The key features that make this operating system more secure are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Based on Red Hat Enterprise Linux: The underlying OS is primarily Red Hat Enterprise Linux components, which means it has the same quality, security, control measures, and support. When a fix is pushed to Red Hat Enterprise Linux, that same fix is pushed to Red Hat Enterprise Linux CoreOS.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Controlled immutability: Red Hat Enterprise Linux CoreOS is managed via OpenShift APIs, which leads to more hands-off operating system management. Management is primarily performed in bulk for all nodes throughout the OpenShift cluster. The latest state of the Red Hat Enterprise Linux CoreOS system is stored on the cluster, making it easy to add new nodes or push updates to all nodes. Given the OS's centralized management and transactional nature, only a few system settings can be modified on a Red Hat Enterprise Linux CoreOS installation.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Command-line container tools: Red Hat Enterprise Linux CoreOS includes container tools compatible with the &lt;a href="https://opencontainers.org"&gt;Open Container Initiative&lt;/a&gt; (OCI) specification to build, copy, and manage container images. Many container runtime administration features are available through Podman. The &lt;a href="https://www.redhat.com/sysadmin/how-run-skopeo-container"&gt;skopeo&lt;/a&gt; command copies, authenticates, and signs images. The &lt;a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/"&gt;crictl&lt;/a&gt; command lets you view and troubleshoot containers and pods.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Robust transactional updates: Red Hat Enterprise Linux CoreOS offer the &lt;a href="https://coreos.github.io/rpm-ostree/"&gt;rpm-ostree&lt;/a&gt; upgrade process, which assures that an upgrade takes place atomically. If something goes wrong, the original OS can be restored in a single rollback.&lt;/p&gt; &lt;p&gt;OpenShift handles OS upgrades through the &lt;a href="https://github.com/openshift/machine-config-operator"&gt;Machine Config Operator&lt;/a&gt; (MCO), which encompasses a complete OS upgrade instead of individual packages as in traditional Yum upgrades. OpenShift also updates nodes via a rolling update to mitigate the updates' impact and maintain cluster capacity. During installation and upgrades, the latest immutable filesystem tree is read from a container image, written to disk, and loaded to the bootloader. The machine will reboot into the new OS version, guaranteeing an atomic update.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Security during cluster installation: Red Hat Enterprise Linux CoreOS minimizes security decisions during installation. Two security features are considered pre-first boot decisions for cluster operations: &lt;a href="https://docs.openshift.com/container-platform/4.8/installing/installing-fips.html#installing-fips"&gt;support for FIPS cryptography&lt;/a&gt; and full disk encryption (FDE). After the cluster is bootstrapped, the cluster can further be configured for other node-level changes.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Container layer&lt;/h2&gt; &lt;p&gt;The container layer in Kubernetes and OpenShift isolates processes from one another and from the underlying OS. Instead of traditional software design, where all the components are linked, deployed together, and ultimately dependent on each other, containers are independent, resulting in smaller impacts. If one container goes down, it can easily be replaced. If a container image is found to have a security flaw, the flaw is isolated to that image and requires updating only that image rather than the whole cluster.&lt;/p&gt; &lt;p&gt;Red Hat OpenShift has many features that improve container security for multitenant environments.&lt;/p&gt; &lt;h3&gt;Container engine&lt;/h3&gt; &lt;p&gt;A &lt;em&gt;container engine&lt;/em&gt; provides tools for creating container images and starting containers. In OpenShift, the default container engine is &lt;a href="https://docs.openshift.com/container-platform/3.11/crio/crio_runtime.html"&gt;CRI-O&lt;/a&gt;, which supports containers conforming to OCI and libcontainerd. The container engine focuses on the features needed by Kubernetes's &lt;a href="https://kubernetes.io/docs/concepts/architecture/cri/"&gt;Container Runtime Interface&lt;/a&gt; (CRI). This customized container engine shrinks the surface available to a security attack, because the container engine does not contain unneeded features such as direct command-line use or orchestration facilities.&lt;/p&gt; &lt;p&gt;We have also aligned the CRI more with Kubernetes: Updates to CRI-O are made to work better with the current Kubernetes release.&lt;/p&gt; &lt;h3&gt;Container security in the Linux kernel&lt;/h3&gt; &lt;p&gt;The kernel offers features to ensure the security of containers and everything else running on the OS. First off, all containers are launched inside a namespace that creates an isolated sandbox segregating the containers, files systems, processes, and networking.&lt;/p&gt; &lt;p&gt;The next feature is &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/ch01"&gt;control groups&lt;/a&gt; (cgroups), which isolate hardware resource sharing between containers and nodes of the OpenShift cluster. The use of cgroups prevents any single process or container from using up all the available resources on a host.&lt;/p&gt; &lt;p&gt;Finally, as we discussed earlier, Red Hat Enterprise Linux CoreOS enables SELinux, which prevents a container from breaking its isolation and thus interfering indirectly with other containers on the same host.&lt;/p&gt; &lt;h2&gt;Cluster security on Kubernetes and Red Hat OpenShift&lt;/h2&gt; &lt;p&gt;The cluster level controls how Kubernetes deploys hosts, manages shared resources, controls intercontainer communications, manages scaling, and controls access to the cluster. An OpenShift cluster is made up of a control plane, worker nodes, and any additional resources needed. The following subsections cover some of the security concerns for the different aspects of the cluster.&lt;/p&gt; &lt;h3&gt;Control plane isolation&lt;/h3&gt; &lt;p&gt;It is considered a best practice to isolate the cluster's control plane nodes from the worker nodes. This is usually done using separate hardware for the control plane to mitigate the impact of any misconfiguration, resource management problems, or vulnerabilities.&lt;/p&gt; &lt;h3&gt;Identity management&lt;/h3&gt; &lt;p&gt;Every Kubernetes cluster needs some form of identity management. Out of the box, Red Hat OpenShift comes with a default &lt;a href="https://oauth.net"&gt;OAuth&lt;/a&gt; provider, which is used for token-based authentication. This provider has a single &lt;code&gt;kubeadmin&lt;/code&gt; user account, which you can use to &lt;a href="https://docs.openshift.com/container-platform/4.10/authentication/understanding-identity-provider.html"&gt;configure an identity provider via a custom resource (CR)&lt;/a&gt;. OpenShift supports &lt;a href="https://openid.net/connect/"&gt;OpenID Connect&lt;/a&gt; and LDAP standard identity providers. After identities are defined, use role-based access control (RBAC) to define and apply permissions.&lt;/p&gt; &lt;h3&gt;Cluster access control&lt;/h3&gt; &lt;p&gt;Before users interact with the cluster, they first must authenticate via the OAuth server. Internal connections to the API server are authenticated using X.509 certificates.&lt;/p&gt; &lt;h3&gt;Security context constraints&lt;/h3&gt; &lt;p&gt;&lt;a href="https://docs.openshift.com/container-platform/4.8/authentication/managing-security-context-constraints.html"&gt;Security context constraints&lt;/a&gt; (SCCs) are an OpenShift security feature that limits a pod's resource access and allowable actions. SCCs let administrators control much of the pod's configuration, such as the SELinux context of a container, whether a pod can run privileged containers, and the use of host directories as volumes. In OpenShift, SCCs are enabled by default and cannot be disabled. SCCs can improve isolation in SaaS deployments and reduce the impact of potential vulnerabilities.&lt;/p&gt; &lt;p&gt;Pod SCCs are determined by the group that the user belongs to as well as the service account, if specified. By default, worker nodes and the pods running on them receive an SCC type of &lt;code&gt;restricted&lt;/code&gt;. This SCC type prevents pods from running as privileged and requires them to run under a UID that is selected at runtime from a preallocated range of UIDs.&lt;/p&gt; &lt;h3&gt;Secrets&lt;/h3&gt; &lt;p&gt;In SaaS deployments, the tenants need to secure their sensitive data on the cluster. This is handled with Secret objects on OpenShift. Secret objects hold sensitive information such as passwords, OCP client configuration files, private source repository credentials, etc. This way of using Secret objects decouples the sensitive content from the pods.&lt;/p&gt; &lt;p&gt;When the sensitive content is needed, it can be mounted to the container via a volume plugin, or the system can use the secrets to perform the action on behalf of the pod. Key properties of secrets include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Secret data can be created by one entity, such as a configuration tool, and referred to by another, such as an application.&lt;/li&gt; &lt;li&gt;Secret data volumes are backed by temporary file-storage facilities (tmpfs) and never come to rest on a node.&lt;/li&gt; &lt;li&gt;Secret data can be shared within a namespace.&lt;/li&gt; &lt;li&gt;Secret data can &lt;a href="https://docs.openshift.com/container-platform/4.10/security/encrypting-etcd.html"&gt;optionally be encrypted at rest&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For more information, read &lt;a href="https://docs.openshift.com/container-platform/4.10/nodes/pods/nodes-pods-secrets.html"&gt;Providing sensitive data to pods&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Red Hat Advanced Cluster Security for Kubernetes&lt;/h3&gt; &lt;p&gt;In addition to the standard security features in Red Hat OpenShift, Red Hat offers additional products to enhance the security of the platform. One of those is &lt;a href="https://cloud.redhat.com/products/kubernetes-security"&gt;Red Hat Advanced Cluster Security for Kubernetes&lt;/a&gt; (previously StackRox). Red Hat Advanced Cluster Security for Kubernetes protects your vital applications across the build, deploy, and runtime stages. It deploys in your infrastructure and easily integrates with &lt;a href="https://developers.redhat.com/topics/devops/"&gt;DevOps&lt;/a&gt; tooling and workflows. This integration makes it easy to apply security and compliance policies.&lt;/p&gt; &lt;p&gt;Red Hat Advanced Cluster Security adds to OpenShift's built-in security by improving the following core tenants of security:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Improving &lt;em&gt;visibility&lt;/em&gt; of the environment, so administrators can more easily detect issues as they happen.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Managing vulnerabilities&lt;/em&gt; once they have been identified by deploying fixes via an integrated &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;CI/CD&lt;/a&gt; pipeline.&lt;/li&gt; &lt;li&gt;Ensuring &lt;em&gt;compliance&lt;/em&gt; with industry standards and best practices.&lt;/li&gt; &lt;li&gt;Adding robust &lt;em&gt;network segmentation&lt;/em&gt; to restrict network traffic to only the necessary uses.&lt;/li&gt; &lt;li&gt;A &lt;em&gt;risk-based &lt;/em&gt;ranking of each deployment to determine the likelihood of a security risk, helping to ensure that the highest risk deployments get immediate remediation first.&lt;/li&gt; &lt;li&gt;Identifying misconfigurations and evaluating role-based access control (RBAC) access for users via &lt;em&gt;configuration management, &lt;/em&gt;to ensure that the configuration meets best practices.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Runtime detection and response&lt;/em&gt; to automatically identify abnormal actions that could indicate a security breach or misuse of the environment.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To learn more, see &lt;a href="https://cloud.redhat.com/blog/a-brief-introduction-to-red-hat-advanced-cluster-security-for-kubernetes"&gt;A Brief Introduction to Red Hat Advanced Cluster Security for Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Networking layer&lt;/h2&gt; &lt;p&gt;The networking layer is the outermost layer of a security architecture. The network is where most IT security attacks occur, due to misconfiguration and vulnerabilities. Proper planning and configuration of the network security layer components ensure that the environment is secure. Kubernetes has software-defined networking (SDN) controls that can improve network security in SaaS deployments. Red Hat OpenShift provides additional controls that build on what's available in Kubernetes.&lt;/p&gt; &lt;h3&gt;Network policy&lt;/h3&gt; &lt;p&gt;A network policy controls the traffic between pods by defining the permissions they need in order to communicate with other pods and network endpoints. OpenShift expands on policies by logically grouping components and rules into collections for easy management.&lt;/p&gt; &lt;p&gt;It is worth noting that network policies are additive. Therefore, when you create multiple policies on one or more pods, the union of all rules is applied regardless of the order in which you list them. The resulting pod behavior reflects every allow and deny rule for ingress and egress.&lt;/p&gt; &lt;h3&gt;Container network interface&lt;/h3&gt; &lt;p&gt;In a Kubernetes cluster, by default, pods are attached to a single network and have a single container network interface (CNI). The CNI manages the network connectivity of containers and removes resources when containers are deleted.&lt;/p&gt; &lt;p&gt;Kubernetes uses SDN plugins to implement the CNI. They manage the resources of the network interfaces for new pods. The CNI plugins set up proper networking constructs for pod-to-pod and pod-to-external communication and enforce network policies.&lt;/p&gt; &lt;h3&gt;Openshift networking security features&lt;/h3&gt; &lt;p&gt;OpenShift offers the following additional features and components to secure networks for cloud-native deployments:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Network operations: OpenShift includes a set of operators that manage networking components to enforce best practices and mitigate human errors.&lt;/li&gt; &lt;li&gt;Multiple network interfaces: The Kubernetes default is for all pods to use a single network and a single primary network interface, but with OpenShift, you can configure additional network interfaces. This allows network optimization to improve performance and enhances isolation to improve security.&lt;/li&gt; &lt;li&gt;Ingress security enhancements: OpenShift exposes the cluster to external resources or clients via a &lt;em&gt;route&lt;/em&gt; resource. Routes provide advanced features not found in a standard Kubernetes Ingress controller, including TLS re-encryption, TLS passthrough, and split traffic for blue-green deployments.&lt;/li&gt; &lt;li&gt;Egress security enhancements: While the default OpenShift rule allows all egress traffic to leave the cluster with no restrictions, OpenShift has tools for fine-grained control and filtering of outbound traffic. OpenShift lets you control egress traffic via an &lt;a href="https://docs.openshift.com/container-platform/4.6/networking/openshift_sdn/configuring-egress-firewall.html"&gt;egress firewall&lt;/a&gt;, &lt;a href="https://docs.openshift.com/container-platform/4.6/networking/openshift_sdn/using-an-egress-router.html"&gt;egress routers&lt;/a&gt;, and &lt;a href="https://docs.openshift.com/container-platform/4.7/networking/openshift_sdn/assigning-egress-ips.html"&gt;egress static IP addresses&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Service mesh: Red Hat OpenShift Service Mesh, based on the &lt;a href="https://istio.io"&gt;Istio&lt;/a&gt; project, adds a transparent layer to existing application network services running in a cluster, allowing complex management and monitoring without requiring changes to the services. The service mesh does this by deploying a sidecar proxy alongside the relevant services to intercept and manage all network communications. With Red Hat OpenShift Service Mesh, you can create a network with the following services: discovery, load balancing, service-to-service authentication, failure recovery, metrics, monitoring, A/B testing, canary releases, rate limiting, access control, and end-to-end authentication.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For more information, see the &lt;a href="https://www.redhat.com/rhdc/managed-files/cl-openshift-security-guide-ebook-us287757-202103.pdf"&gt;Red Hat OpenShift Security Guide&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Partner with Red Hat to build your SaaS&lt;/h2&gt; &lt;p&gt;This article covered controls that can be used to improve the security of your SaaS deployment at the hardware, OS, container, Kubernetes cluster, and network levels. Future articles will go deeper into SaaS security topics.&lt;/p&gt; &lt;p&gt;&lt;a href="https://connect.redhat.com/en/partner-with-us/red-hat-saas-foundations"&gt;Red Hat SaaS Foundations&lt;/a&gt; is a partner program designed for building enterprise-grade SaaS platforms on Red Hat OpenShift or Red Hat Enterprise Linux, and deploying them across multiple cloud and non-cloud footprints. &lt;a href="http://https//mail.google.com/mail/?view=cm&amp;fs=1&amp;tf=1&amp;to=saas@redhat.com"&gt;Email&lt;/a&gt; us to learn more.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/27/saas-security-kubernetes-environments-layered-approach" title="SaaS security in Kubernetes environments: A layered approach"&gt;SaaS security in Kubernetes environments: A layered approach&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Alex Kubacki</dc:creator><dc:date>2022-07-27T07:00:00Z</dc:date></entry><entry><title type="html">Vert.x virtual threads incubator</title><link rel="alternate" href="https://vertx.io/blog/vertx-virtual-threads-incubator" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/vertx-virtual-threads-incubator</id><updated>2022-07-27T00:00:00Z</updated><content type="html">JEP 425: Virtual threads aka Loom is coming to the Java Platform as a preview in Java 19. The Vert.x team is launching an incubator project to experiment with virtual threads.</content><dc:creator>Julien Viet</dc:creator></entry><entry><title type="html">How to use Java Mission Control to monitor Java apps</title><link rel="alternate" href="http://www.mastertheboss.com/java/how-to-use-java-mission-control-to-monitor-java-apps/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/how-to-use-java-mission-control-to-monitor-java-apps/</id><updated>2022-07-25T07:37:43Z</updated><content type="html">This article is whirlwind tour across the Java Flight Recorder (JFR) and the Java Mission Control (JMC) suite. At the end of it, you will be able to monitor, collect diagnostic data and profile any running Java application. What is Java Flight Recorder? Firstly, why do we need another tool to monitor Java ? As ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">RESTEasy Releases</title><link rel="alternate" href="https://resteasy.github.io/2022/07/21/resteasy-releases/" /><author><name /></author><id>https://resteasy.github.io/2022/07/21/resteasy-releases/</id><updated>2022-07-21T18:11:11Z</updated><dc:creator /></entry><entry><title>How to use Go Toolset container images</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/21/how-use-go-toolset-container-images" /><author><name>Alejandro Sáez Morollón</name></author><id>011e5016-34ab-4570-8e1a-575cc4281eec</id><updated>2022-07-21T07:00:00Z</updated><published>2022-07-21T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_developer_tools/2019.1/html-single/using_go_toolset/index"&gt;Go Toolset package&lt;/a&gt; delivers the &lt;a href="https://developers.redhat.com/topics/go"&gt;Go language&lt;/a&gt; with Federal Information Processing Standard (FIPS) support for cryptographic modules and the &lt;a href="https://github.com/go-delve/delve"&gt;Delve debugger&lt;/a&gt; to &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; customers. We introduced this package &lt;a href="https://developers.redhat.com/blog/2017/10/31/getting-started-go-toolset"&gt;a few years ago&lt;/a&gt;. Now we also provide Go Toolset in container images. This article illustrates how these images support modern Go development.&lt;/p&gt; &lt;h2&gt;Obtaining Go Toolset container images&lt;/h2&gt; &lt;p&gt;The images are in Red Hat's &lt;a href="https://catalog.redhat.com/software/containers/explore"&gt;container image catalog&lt;/a&gt;. Search for &lt;code&gt;go-toolset&lt;/code&gt; &lt;a href="https://catalog.redhat.com/software/containers/search?q=go-toolset"&gt;here&lt;/a&gt;. As of this writing, we have images based on Red Hat Enterprise Linux 7, Red Hat Enterprise Linux 8, and &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat Universal Base Images&lt;/a&gt; (UBI) 7 and 8. To learn more about UBI, check the article &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat UBI is a Verified Publisher on Docker Hub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you don't want to use the Go Toolset container image, you can still install the Go Toolset package inside a ubi8 container. Just run &lt;code&gt;dnf install go-toolset&lt;/code&gt; inside the &lt;code&gt;registry.access.redhat.com/ubi8&lt;/code&gt; image, and you'll be all set.&lt;/p&gt; &lt;h2&gt;Pull the latest version of the Go Toolset image&lt;/h2&gt; &lt;p&gt;Using your container engine is the best way to pull down a container image. Let's use &lt;a href="https://podman.io"&gt;Podman&lt;/a&gt; for this example. The following commands pull down the latest version of the image based on UBI 8 and run a shell inside it:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;[alex@lab ~]$ podman pull registry.access.redhat.com/ubi8/go-toolset:latest [alex@lab ~]$ podman run --rm -it go-toolset /bin/bash bash-4.4$ go version go version go1.17.7 linux/amd64 bash-4.4$ exit exit [alex@lab ~]$ &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Using the image in a multistage environment&lt;/h2&gt; &lt;p&gt;You can refer to the image in a Dockerfile like any other image and use it, for example, as a build step in a multistage Dockerfile in tandem with the &lt;a href="https://catalog.redhat.com/software/containers/ubi8/ubi-micro/5ff3f50a831939b08d1b832a"&gt;Red Hat Universal Base Image 8 Micro image&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="Dockerfile"&gt;FROM ubi8/go-toolset as build COPY ./src . RUN go mod init my_app &amp;&amp; \ go mod tidy &amp;&amp; \ go build . FROM ubi8/ubi-micro COPY --from=build /opt/app-root/src/my_app . CMD ./my_app &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The ubi-micro image takes up less than 40MB, so the surface of your container is tiny but holds all of the great features UBI delivers.&lt;/p&gt; &lt;h2&gt;Go Toolset works with Toolbox&lt;/h2&gt; &lt;p&gt;I won’t be lying if I say &lt;a href="https://github.com/containers/toolbox"&gt;Toolbox&lt;/a&gt; is one of my favorite software tools. It allows you to keep working with your files and configurations inside a new container. I use Toolbox every day, and it works wonderfully with the Go Toolset image.&lt;/p&gt; &lt;p&gt;You can install Toolbox in both Red Hat Enterprise Linux and Fedora with &lt;code&gt;dnf install toolbox&lt;/code&gt;. The &lt;code&gt;toolbox&lt;/code&gt; command lets you install other resources:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;[alex@lab ~]$ cat /etc/redhat-release Fedora release 35 (Thirty Five) [alex@lab ~]$ toolbox create --image registry.access.redhat.com/ubi8/go-toolset [alex@lab ~]$ toolbox enter go-toolset [alex@toolbox ~]$ cat /etc/redhat-release Red Hat Enterprise Linux release 8.6 (Ootpa) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you have all the files, configurations, and packages included with the image.&lt;/p&gt; &lt;h2&gt;Attach VS Code for IDE support&lt;/h2&gt; &lt;p&gt;You can improve productivity by attaching Visual Studio Code, &lt;a href="https://developers.redhat.com/products/vscode-extensions/overview"&gt;VS Code&lt;/a&gt; to a running container. Install the &lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers"&gt;Remote Containers extension&lt;/a&gt; and execute &lt;strong&gt;Attach to Running Container&lt;/strong&gt;. There is no need to configure Git or Kerberos tokens; simply jump into the container and start working.&lt;/p&gt; &lt;p&gt;I use this process to play with the new versions of the container images we built and bootstrap projects such as Go.&lt;/p&gt; &lt;h2&gt;Go Toolset includes FIPS security&lt;/h2&gt; &lt;p&gt;One exciting feature supported by the golang package in Go Toolset is &lt;a href="https://www.redhat.com/en/blog/how-rhel-8-designed-fips-140-2-requirements"&gt;FIPS 140-2 cryptographic modules&lt;/a&gt;. You can expect Go Toolset to follow the FIPS 140-2 security standard. Check for FIPS mode inside the Go Toolset container image:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;[alex@lab ~]$ fips-mode-setup --check FIPS mode is enabled. [alex@lab ~]$ toolbox enter go-toolset [alex@toolbox ~]$ fips-mode-setup --check FIPS mode is enabled. &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Enterprise-ready Go in Red Hat Enterprise Linux&lt;/h2&gt; &lt;p&gt;The Go Toolset package is available as an image and integrates smoothly with other popular developer tools. By following the instructions I have provided, you can quickly become a more productive Go programmer in the cloud.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/21/how-use-go-toolset-container-images" title="How to use Go Toolset container images"&gt;How to use Go Toolset container images&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Alejandro Sáez Morollón</dc:creator><dc:date>2022-07-21T07:00:00Z</dc:date></entry><entry><title type="html">An improved &amp;#8216;Spreadsheet like&amp;#8217; experience on DMN Editor</title><link rel="alternate" href="https://blog.kie.org/2022/07/a-better-spreadsheet-like-experience.html" /><author><name>Fabrizio Antonangeli</name></author><id>https://blog.kie.org/2022/07/a-better-spreadsheet-like-experience.html</id><updated>2022-07-20T14:33:15Z</updated><content type="html">The boxed expression editor is a key component of the DMN Editor. In previous articles, we introduced the new implementation of this . In this article, I’ll show how I extended the component, implementing the keyboard navigation for faster DMN editing. REQUIREMENTS * (1.46.0+); * (0.20.0+); , there is a ready-to-use online version of the DMN editor to try the new functionality. THE NEW KEYBOARD NAVIGATION The editing of a decision in the DMN Editor was based on the mouse interaction requiring the user to continuously switch between keyboard and mouse, resulting in a time-consuming activity. As my first task on the project, I worked on implementing a user experience as much as possible similar to Google Spreadsheet. As a result, the user can edit an expression, cell by cell, seamlessly using only the keyboard. THE NEW KEYBOARD ACTIONS AVAILABLE IN “VIEW MODE” Navigation between cells is now available in any type of expression using the arrow keys to go UP, RIGHT, DOWN, and LEFT, in a natural way. Continuous navigation is available with TAB, to jump to the next cell, or SHIFT-TAB to jump to the previous cell. This with the exception that if you are at the end of the row, you jump to the first cell of the next row or the last cell of the previous row if you are on the first cell of the row. After you choose the cell you want to edit, you just start writing the content, and this way, you erase the already existing content, if there is any. In addition, if you just want to start editing from the end of the cell’s content, you just need to press ENTER and start typing your content. Differently from a normal Spreadsheet, we have particular cells with nested tables or cells that don’t have just text, and when you click on them, a pop-up with a few inputs will appear, used in different cases. In this last case, you can now open the pop-up by pressing ENTER, and then you jump between the inputs inside the form using TAB/SHIFT-TAB. ENTER/ESC will close the pop-up and save or cancel your changes. For the case of nested tables, for instance, a "Context expression" with a Decision Table inside, you can jump inside the nested table with ENTER key and come back to the parent table with ESC. THE NEW KEYBOARD ACTIONS AVAILABLE IN “EDIT MODE” When you finish editing a cell, and you want to apply your changes, using TAB/SHIFT-TAB you save and jump directly to the next or previous cell, or to the cell below with ENTER. On the contrary, you can press ESC to cancel your changes to the cell. A big change to the UX was the introduction of the "newline" in the Decision Table’s cells, using the CTRL-ENTER. This change impacted the logic of the "copy &amp;amp; paste" that was based on single-line cells. To achieve that, the new logic is based on the parser, and now you can just copy your data to or from a Spreadsheet. The implementation The work has been mainly focused on the package inside repository. The main obstacle to implementing all these functionalities was the communication between React custom and third-party components and highlighting the selected cell. The easiest way was possibly the use of the “contenteditable” attribute, but that required a full rewriting of the components for the table and cells. After an evaluation of 4 other solutions, we decided to listen to the keyboard events from the TD HTML element representing the cell, then show the highlight through the “:focus” CSS selector. This way managing the "onBlur" event or memorizing the selected cell is not needed. Instead, when the focus is on the input, which is actually a Monaco editor, the highlight needs to be on the component inside the cell, that has the state data. This is done by adding the CSS class "editable-cell–edit-mode" to the main tag of the component EditableCell. Then to ensure the stability of the component we use "Jest" and "@testing-library/react" to render components. For the E2E test, we use "Cypress", which currently doesn’t support TAB key simulation which we managed with the plugin. CONCLUSION Creating or modifying a Decision can be a time-consuming activity if you have a lot of data to add inside. Giving the user the ability to write that data quickly and in the way, he was used to, was very important to us. We also didn’t want the user to learn a new way for that, and we wanted to give the same experience as Google Spreadsheet or Excel. The post appeared first on .</content><dc:creator>Fabrizio Antonangeli</dc:creator></entry></feed>
